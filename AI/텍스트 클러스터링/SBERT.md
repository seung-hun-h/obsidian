## 1. 정의
**SBERT(Sentence-BERT)** 는 사전 훈련된 [[BERT]] 모델을 문장 임베딩 생성에 특화시킨 모델이다. 문장 간의 의미적 유사도 비교, 검색, 클러스터링과 같은 작업을 매우 효율적이고 정확하게 수행하는 것을 목표로 한다.

## 2. 필요성
SBERT는 [[BERT]]의 계산 비효율성, 성능의 한계를 해결하기 위해 나타났다

**BERT의 한계**
- 계산의 비효율성
	- 두 문장의 유사도를 측정하려면 항상 두 문장을 쌍으로 묶어 [[BERT]]에 입력해야 했다
	- 10,000개의 문장이 있다면 모든 쌍을 비교하기위해 약 5천만 번의 계산이 필요하다
- 성능의 한계
	- [[BERT]]의 [[BERT#1. CLS(Classification) 토큰|CLS 토큰]]은 유사도 측정에 최적화된 것이 아니라 다음 문장 예측과 같은 작업에 맞춰져있다
	- 의미가 비슷한 문장들의 벡터가 정작 벡터 공간에서는 가깝지 않은 문제가 발생했다
## 3. SBERT의 구조 및 학습 방식
SBERT는 [[BERT]]의 한계를 해결하기 위해 **폴링 레이어**를 추가하고, **샴 네트워크나 삼중항 네트워크** 구조의 지도 학습 방식을 사용하여 파인 튜닝한다.
### 1. 폴링 레이어
SBERT는 폴링 레이어를 통해 어떤 길이의 문장이 입력되더라도 항상 동일한 차원의 문장 임베딩 벡터 하나를 얻을 수 있다
- [[BERT]] 레이어
	- 입력된 문장의 각 단어에 대한 풍부한 문맥적 의미가 담긴 벡터를 출력하는 역할
- 폴링 레이어
	- [[BERT]] 레이어에서 출력한 여러 개의 토큰 벡터로부터 문장 전체를 대표하는 하나의 고정 크기 벡터를 추출
	- 여러 가지 방식이 있지만, 일반적으로 모든 토큰 벡터의 값을 차원별로 평균내는 **평균 폴링** 방식이 가장 널리 쓰인다
### 2. 샴 네트워크
- 학습 데이터
	- 두개의 문장과 그 문장의 관계를 나타내는 정답(레이블)이 있는 데이터셋
- 학습 과정
	1. 한 쌍의 문장을 동일한 가중치를 공유하는 SBERT 모델에 각각 통과시켜 두 개의 문장 임베딩 벡터(`u`, `v`)를 얻는다
	2. 두 임베딩 벡터 `u`와 `v`의 코사인 유사도를 계산한다
	3. 모델이 계산한 유사도와 실제 데이터의 정답 레이블을 비교하여, 그 차이를 줄이는 방향으로 모델 전체의 가중치를 업데이트한다
### 3. 삼중항 네트워크
- 학습 데이터
	- 기준이 되는 앵커 문장, 앵커와 의미가 유사한 긍정 문장, 앵커와 의미가 다른 부정 문장 세 개가 한 세트인 데이터를 사용한다
- 학습 과정
	1. 세 문장을 동일한  SBERT 모델에 통과시켜 각각의 임베딩 벡터를 얻는다
	2. 앵커와 긍정 임베딩 사이의 거리는 최대한 가깝게, 앵커와 부정 임베딩 사이의 거리는 최대한 멀어지도록 학습한다

**BERT의 한계 해결**
- 모든 쌍을 비교하지 않고, 각 문장을 한 번만 [[BERT]] 레이어, 폴링 레이어를 거쳐 임베딩 벡터 하나를 얻는다. 이후 유사도 비교는 만들어둔 임베딩 벡터를 가지고 코사인 유사도 등 가벼운 수학 연산으로 수행하여 시간을 단축 시켰다
- '유사도 비교'라는 목표를 가진 지도 학습(샴 네트워크, 삼중항 네트워크)을 통해 의미적 유사도가 벡터간 거리로 잘 표현되도록 파인튜닝했다
