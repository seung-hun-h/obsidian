## 1. 정의
BERT (Bidirectional Encoder Representations from Transformers)는 문장의 문맥을 **양방향으로 이해**하도록 만든 사전 학습 기반 언어모델이다. 즉, 문장의 의미를 더 정확하게 파악하기 위해 좌우 문맥을 동시에 고려하는 모델이다.

기존 모델들은 보통
`나는 밥을 ___`처럼 왼쪽에서 오른쪽으로만 문장을 이해했다. 하지만 자연어는 좌우를 모두 살펴 문맥을 파악해야 한다
```text
강변 은행에서 돈을 찾았다 -> 금융기관
강변에 떨어진 은행을 주웠다 -> 열매
```
BERT는 기존 모델과 다르게 양방향의 문맥을 동시에 고려한다.

## 2. 학습 방식
### 1. MLM (Masked Language Modeling)
문장 속 일부 단어를 `[MASK]` 토큰으로 가리고, 나머지 문맥을 통해 가려진 단어를 예측하도록 학습하는 방식이다

```text
나는 [MASK]를 좋아한다
```
모델은 `나는`, `를`, `좋아한다`라는 문맥을 보고 `[MASK]` 자리에 들어갈 단어가 `커피`라는 것을 맞혀야한다.
양방향 문맥을 모두 파악해야 하므로 모델이 단어 간 관계와 문맥 패턴을 깊게 학습할 수 있다.
### 2.  NSP(Next Sentence Prediction)
두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장의 다음 문장이 맞는지 예측하는 방식이다. 문장간 연결성을 학습하고 문장 수준의 문맥 이해에 도움을 준다.

실제 다음 문장일 경우(label = 1)
```text
[문장 1]: 나는 커피를 마셨다.  
[문장 2]: 그래서 밤에 잠이 안 왔다.
```

무작위 문장일 경우(label = 0)
```text
[문장 1]: 나는 커피를 마셨다.  
[문장 2]: 내일 날씨가 맑을 예정이다.
```

문장 간의 관계를 이해하면 **질의응답, 요약, 자연스러운 문장 생성** 등에 매우 유리하며 **문장 단위의 논리 흐름**을 파악할 수 있다.

## 3. CLS(Classification) 토큰
BERT는 입력 문장의 맨 앞에 항상 `[CLS]`라는 클래스 분류용 토큰을 붙인다. 이 토큰은 모델이 문장의 의미를 요약해서 담는 용도로 사용한다.

입력
```text
입력: 나는 커피를 마신다
BERT: [CLS] 나는 커피를 마신다
```

Transformer 인코딩
```text
[CLS] -> [0.12, 0.85, ... , -0.14] <- 문장 전체 정보 요약
나는 -> [...]
커피를 -> [...]
마신다 -> [...]
```

`[CLS]` 벡터만 뽑아서 분류기(fully-connected layer)에 입력
```
cls_vector = output[0]  # [CLS] 위치 = index 0
logits = classifier(cls_vector)  # Softmax 통해 클래스 예측
```

