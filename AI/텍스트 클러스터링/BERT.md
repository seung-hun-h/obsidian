## 1. 정의
BERT (Bidirectional Encoder Representations from Transformers)는 [[트랜스포머]]의 [[트랜스포머#3. 주요 구조|인코더]] 구조를 기반으로 하며, 문장 내 문맥을 **양방향(Bidirectional)** 으로 깊이 있게 이해하도록 설계된 사전 훈련 언어 모델이다.

특정 작업(번역, 텍스트 생성)을 직접 수행하기 보다는, 다양한 자연어 처리(NLP) 작업의 기초가 될 수 있는 **범용적인 언어 표현** 자체를 학습하는 것이 목표이다. 즉, 언어 자체에 대한 깊은 이해를 목표로한다

**양방향**이란 문장 내 특정 단어의 의미를 파악할 때, 해당 단어의 왼족에 있는 단어들 뿐 아니라 오른쪽에 있는 단어들까지 동시에 고려하여 문맥을 파악한다는 의미이다.
- "그녀는 은행에서 돈을 찾았다" VS "강가에 떨어진 은행을 주웠다". BERT는 은행이라는 단어의 의미를 파악하기 위해 앞뒤 문맥('돈을 찾았다', '강가에 떨어진')을 고려하기 때문에 은행이 서로 다른 의미(금융기관, 열매)를 가지는 것을 알 수 있다 

## 2. 트랜스포머 아키텍처의 확장
- BERT는 [[트랜스포머]] 아키텍처를 확장한 것이라고 볼 수 있다
- 트랜스포머 인코더 채택
	- 트랜스포머의 인코더-디코더 구조 중에서 인코더 부분만을 선택적으로 사용하고, 이를 여러 층으로 깊게 쌓아 올린다
	- 트랜스포머의 셀프 어텐션 매커니즘은 문장 내 단어들의 관계를 파악하고 문맥을 이해하는데 효과적인데, BERT는 이 능력을 언어 자체의 표현을 학습하는데 집중적으로 활용한다
- 혁신적인 사전 학습 방식 도입
	- 트랜스포머 자체는 특정 작업을 위해 처음부터 레이블된 데이터로 학습될 수 있는 구조를 제공했다
	- BERT는 트랜스포머의 인코더 구조를 활용하여, 레이블이 없는 방대한 텍스트 데이터로부터 언어 자체의 패턴과 지식을 학습할 수 있는 두 가지 독창적인 사전 학습 방법을 제안했다
		- MLM(Masked Language Model): 문장 내 일부 단어를 가리고 맞추게 함으로써, 단어의 의미를 양방향 문맥 속에서 깊이 이해하도록 학습한다
		- NSP(Next Sequence Prediction): 두 문장이 실제로 이어지는 내용인지 예측하게 함으로써, 문장 간의 관계를 이해하도록 학습한다
	- 이러한 사전 학습 방식을 통해 BERT는 특정 작업에 국한되지 않는 매우 풍부하고 일반적인 언어 이해 능력을 갖추게된다
- 특정 입력 표현 방식 구체화
	- 효과적인 사전 학습과 다운스트림 작업을 위해 입력 텍스트를 특정 형식으로 구성한다
		- `[CLS]` 토큰: 문장의 시작을 알리고, 문장 전체의 특징을 집약하여 분류 작업 등에 활용된다
		- `[SEP]` 토큰: 여러 문장을 구분하는 역할을 한다
		- 세그먼트 임베딩: NSP와 같이 두 문장을 입력으로 사용할 때, 각 토큰이 어느 문장에 속하는지를 구분해주는 정보를 추가한다
- 사전 학습 후 파인튜닝 패러다임
	- 방대한 데이터로 사전 학습된 BERT 모델 위에, 풀고자하는 특정 작업에 맞는 작은 레이어만 붙이고 해당 작업의 데이터로 약간만 더 학습(파인튜닝)하면 매우 높은 성능을 얻을 수 있다는 것을 보여주었다
## 3. 사전 학습 방식
### 1. MLM (Masked Language Modeling)
문장 속 일부 단어를 `[MASK]` 토큰으로 가리고, 나머지 문맥을 통해 가려진 단어를 예측하도록 학습하는 방식이다

```text
나는 [MASK]를 좋아한다
```
모델은 `나는`, `를`, `좋아한다`라는 문맥을 보고 `[MASK]` 자리에 들어갈 단어가 `커피`라는 것을 맞혀야한다.
양방향 문맥을 모두 파악해야 하므로 모델이 단어 간 관계와 문맥 패턴을 깊게 학습할 수 있다.
### 2.  NSP(Next Sentence Prediction)
두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장의 다음 문장이 맞는지 예측하는 방식이다. 문장간 연결성을 학습하고 문장 수준의 문맥 이해에 도움을 준다.

실제 다음 문장일 경우(label = 1)
```text
[문장 1]: 나는 커피를 마셨다.  
[문장 2]: 그래서 밤에 잠이 안 왔다.
```

무작위 문장일 경우(label = 0)
```text
[문장 1]: 나는 커피를 마셨다.  
[문장 2]: 내일 날씨가 맑을 예정이다.
```

문장 간의 관계를 이해하면 **질의응답, 요약, 자연스러운 문장 생성** 등에 매우 유리하며 **문장 단위의 논리 흐름**을 파악할 수 있다.


## 4. BERT의 활용
### 1. CLS(Classification) 토큰
BERT는 입력 문장의 맨 앞에 항상 `[CLS]`라는 클래스 분류용 토큰을 붙인다. 이 토큰은 모델이 문장의 의미를 요약해서 담는 용도로 사용한다. 
- NSP 사전 학습: 토큰의 최종 출력 벡터가 두 문장간의 관계를 예측하는데 사용된다. 
- 파인튜닝: 문장 전체의 의미를 집약한 표현으로 간주되어, 특히 문장 분류작업의 입력으로 주로 활용된다. 

입력
```text
입력: 나는 커피를 마신다
BERT: [CLS] 나는 커피를 마신다
```

Transformer 인코딩
```text
[CLS] -> [0.12, 0.85, ... , -0.14] <- 문장 전체 정보 요약
나는 -> [...]
커피를 -> [...]
마신다 -> [...]
```

`[CLS]` 벡터만 뽑아서 분류기(fully-connected layer)에 입력
```
cls_vector = output[0]  # [CLS] 위치 = index 0
logits = classifier(cls_vector)  # Softmax 통해 클래스 예측
```

### 2. 출력 임베딩 및 파인튜닝
BERT의 마지막 트랜스포머 인코딩 층인 입력 시퀀스의 각 토큰에 대한 최종적인 **문맥적 임베딩**을 출력한다. 이 임베딩은 각 단어가 가지는 풍부한 의미를 담고 있다.
이러한 문맥적 임베딩을 특정 다운스트림 작업의 입력으로 사용하여, 해당 작업에 맞게 모델 전체 또는 일부를 추가적으로 학습시키는 과정을 '파인튜닝'이라 한다

## 5. 단점
- BERT에서 문장 전체의 임베딩을 얻어 문장 간 유사도를 직접 비교하거나 클러스터링에 활용하려고 할 때, 계산 비용이 크거나 `[CLS]` 토큰 또는 풀링 방식만으로는 의미적으로 가장 유사한 문장을 찾아내는데 최적의 성능을 내지 못할 수도 있다
- BERT의 문장 임베딩 생성 방식의 한계를 개선하여, 더 효율적이면서도 의미적으로 우수한 고품질 문장 임베딩을 생성하는 것을 목표로 하는 모델이 [[SBERT]](Sentence BERT)이다