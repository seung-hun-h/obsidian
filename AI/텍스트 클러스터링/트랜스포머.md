## 1. 정의
2017년 구글 논문 "Attention Is All You Need"에서 발표된, 주로 자연어 처리 분야에서 사용되는 딥러닝 모델 아키텍처이다. 기존의 모델들이 가지고 있는 장기 의존성 문제와 순차적 계산으로 인한 병렬 처리 문제를 해결하기 위해 등장하였다. **어텐션 매커니즘**을 주요 구조로 사용한다.

> [!info] 장기 의존성
> AI 모델이 문장이나 이야기 속에서 서로 멀리 떨어져있는 단어나 정보들 간의 관계를 제대로 기억하고 연결짓기 힘들어하는 현상

## 2. 핵심원리
- **어텐션**: 입력 시퀀스에서 특정 요소를 처리할 때, 전체 시퀀스 내의 다른 요소들 중 어떤 것에 더 주목하여 정보를 선택적으로 활용할 지 결정하는 매커니즘
- **셀프 어텐션**
	- 입력 시퀀스 내부에서 단어들끼리 서로 관계와 중요도를 계산하여 문맥 정보를 추출하는 방식
	- 쿼리, 키, 밸류로 구성된다
		- 쿼리(Query): 현재 분석하려는 단어(정보)
		- 키(Key): 문장 내 다른 단어들이 가진 색인 또는 특징
		- 밸류(Value): 다른 단어들이 가진 실제 의미 또는 내용
		- 쿼리가 모든 키들과의 유사도(어텐션 스코어)를 계산하고, 이 스코어를 가중치 삼아 밸류들의 가중합을 구해 현재 쿼리 단어의 문맥적 표현을 생성한다.
- **멀티 헤드 어텐션**
	- 어텐션을 여러 개 병렬로 수행하여 다양한 측면의 문맥 정보를 동시에 포착하는 방식이다. 이를 통해 더 풍부한 표현 학습이 가능해진다.
### 예시
문장: "AI가 세상을 바꾼다"
1. 현재 처리 대상 단어(Query): "AI"
	- 쿼리(`Q(AI)`): "AI"는 현재 자신의 벡터 표현이다. "AI"는 스스로에게 "이 문장에서 나와 관련된 중요한 정보는 무엇일까?" 라고 질문한다
2. 문장내 다른 단어들의 역할(Keys, Values)
	- 문장 내 모든 단어는 각각 키(K)와 밸류(V) 벡터를 가진다
	- 키(K): 각 단어의 라벨 혹은 색인이다. 단어가 가지는 특징을 벡터화 한 것이다. 쿼리는 이 키들과 자신을 비교하여 얼마나 관련있는지 판단한다.
		- `K(AI)`: "AI"가 가지는 특징을 벡터화한 결과.
		- `K(가)`: "가" 가 가지는 특징을 벡터화한 결과.
	- 밸류(V):  각 단어가 가지는 실제 의미 또는 내용이다. 쿼리와 키의 관계에 따라 이 밸류들이 조합되어 쿼리의 새로운 의미를 형성한다.
		- `V(AI)`: "AI" 밸류 벡터. 실제 의미를 담고 있음
		- `V(가)`: "가"의 밸류 벡터
3. 어텐션 과정
	- **스코어 계산**: `Q(AI)`는 문장내 모든 단어의 키들과 자신의 관련도 점수(어텐션 스코어)를 계산한다.
	- **가중치 부여(Softmax)**: 계산된 어텐션 스코어들은 소프트맥스 함수를 거쳐 총합이 1인 확률값으로 변환된다. 어떤 단어의 의미를 얼마나 많이 가져올 지를 결정하는 비율이다.
	- **밸류의 가중합**: 각 단어의 `V` 벡터에 해당 어텐션 가중치를 곱한 후 이들을 모두 더한다
		- 새로운_표현(AI) = (가중치(AI에 대한 AI) * V(AI)) + (가중치(AI에 대한 가) * V(가)) + (가중치(AI에 대한 세상을) * V(세상을)) + (가중치(AI에 대한 바꾼다) * V(바꾼다))
4. 결과: "AI"의 새로운 문맥적 표현
	- 새로운 문맥적 표현은 "AI가 세상을 바꾼다" 내에서 다른 단어와의 관계, 특히 "세상을 바꾼다"는 핵심 문맥 정보를 강하게 반영하게 된다.
## 3. 주요 구조
- **인코더-디코더 구조**
	- 인코더: 입력 시퀀스를 받아 문맥 정보를 압축한 표현(벡터 시퀀스)으로 변환. 문장을 이해하는 것
		- 주요 구성 요소: 셀프 어텐션층 등
	- 디코더: 인코더의 출력과 이전에 생성된 출력값을 입력으로 받아 다음 출력 시퀀스의 요소를 생성. 문장을 생성하는 것
		- 주요 구성 요소: 셀프 어텐션층, 인코더-디코더 어텐션층 등
	- [[BERT]] 같은 모델은 문장을 이해하는 것이 목표이므로 인코더를 선택하여 사용한다.
- **병렬 처리**
	- 입력 시퀀스 내 모든 단어에 대한 계산을 동시에 수행할 수 있다
- **다층 구조**
	- 인코더와 디코더는 각각 여러 개의 동일한 층을 쌓아 구성되며, 층을 거듭할 수록 더 복잡하고 추상적인 특징 학습
## 4. 입력 표현 방식
- 토큰 임베딩(Token Embeddings): 단어나 서브워드 단위로 분리된 토큰을 초기 벡터로 변환
- 위치 임베딩(Positional Embeddings): 셀프 어텐션은 단어의 순서 정보를 직접적으로 반영하지 못하므로, 각 토큰의 시퀀스 내 위치 정보를 벡터로 만들어 토큰 임베딩에 더해줌
## 5. 의의 및 영향
- 장기 의존성 문제 해결: 문장 내 멀리 떨어진 단어 간의 관계도 효과적으로 학습
- NLP 분야의 혁신: BERT, GPT, T5 등 수많은 고성능 언어 모델의 기반 아키텍처가 됨
- 다양한 분야로 확장: 자연어처리 뿐만 아니라 컴퓨터 비전, 음성 인식 등 다른 분야에도 활발히 적용중