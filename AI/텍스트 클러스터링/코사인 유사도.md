## 1. 코사인 유사도란?
코사인 유사도는 두 벡터 사이의 **방향 유사성**을 측정하는 방법이다. 두 문장이 비슷한 의미를 가지면 그 문장의 임베딩 벡터의 방향이 비슷해지고 코사인 유사도가 1이된다.

### 코사인 유사도 공식
$$
\text{cosine\_similarity}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \cdot \|\vec{B}\|}
$$
$$
\begin{aligned}
\vec{A} \cdot \vec{B} &: \text{ 벡터의 내적 (dot product)} \\
\|\vec{A}\| &: \text{ 벡터 A의 크기 (L2 norm)} \\
\|\vec{B}\| &: \text{ 벡터 B의 크기}
\end{aligned}
$$

| 값    | 의미                         |
| ---- | -------------------------- |
| 1.0  | 완전히 같은 방향 (매우 유사)          |
| 0.0  | 완전히 직각 (관련 없음)             |
| -1.0 | 완전히 반대 방향 (의미상 반대, 잘 안 나옴) |
## 2. 각도를 사용하는 이유
### 텍스트 임베딩 벡터는 길이가 다를 수 있다
같은 의미를 가진 문장이라도 길이가 다를 수 있다. 예를 들어,
- "배송이 늦어요"
- "이번에도 배송이 늦어 화가 납니다."
위 두 문장은 '배송이 늦다'는 같은 의미를 지니지만 길이가 다르다. 문장이 길어질 수록 벡터의 절대값(길이)가 커지는 경향이 있다

> [!info] 벡터의 절대값 = 길이
> 벡터의 절대값(norm)은 곧 벡터의 길이(크기)를 나타낸다. 수학에서는 벡터의 노름(norm)이라 한다. 흔히 사용하는 것은 유클리디안 거리이다.
> $$
> \|\vec{v}\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}
> $$

