## 1. 출발점: 정보와 불확실성의 관계

### 1.1 정보량의 직관
- **예측 가능성이 높을수록** → 정보량이 **적다**
- **불확실성이 높을수록** → 정보량이 **많다**
- 예시: "내일 해가 뜬다" (정보량 적음) vs "내일 비가 온다" (정보량 많음)

### 1.2 클러스터링에서의 의미
- **좋은 클러스터**: 불확실성이 낮다 (그룹 특성이 명확)
- **나쁜 클러스터**: 불확실성이 높다 (그룹 특성이 애매)

## 2. 엔트로피(Entropy): 불확실성의 수치화

### 2.1 기본 개념
- **엔트로피** = 불확실성을 수학적으로 측정하는 지표
- 확률 분포가 균등할수록 엔트로피가 높음
- 한 곳에 집중될수록 엔트로피가 낮음

### 2.2 수학적 표현
```
H(X) = -Σ P(x) × log₂(P(x))
```
- 각 확률에 대해 "놀라움의 정도" × "그 확률"을 계산하여 평균

### 2.3 실제 예시
- **균등분포** [33%, 33%, 34%] → 엔트로피 **높음** (≈1.58)
- **편중분포** [90%, 5%, 5%] → 엔트로피 **낮음** (≈0.57)

## 3. 상호정보(Mutual Information): 정보 전달량 측정

### 3.1 핵심 아이디어
클러스터 정보를 알면 정답에 대한 불확실성이 얼마나 줄어드는가?

### 3.2 수학적 정의
```
MI(정답, 클러스터) = H(정답) - H(정답|클러스터)
```
- **H(정답)**: 원래 정답에 대한 불확실성
- **H(정답|클러스터)**: 클러스터 정보를 안 후 정답에 대한 불확실성
- **차이**: 클러스터가 제공한 정보량

### 3.3 해석
- **MI = 0**: 클러스터가 정답에 대해 아무 정보도 제공하지 않음
- **MI = H(정답)**: 클러스터만 알면 정답을 완벽 예측 가능

## 4. 정규화(Normalization): 0~1 척도 만들기

### 4.1 필요성
- 상호정보는 절댓값이라 비교가 어려움
- 데이터셋마다 최대 가능한 정보량이 다름

### 4.2 NMI 공식
```
NMI = MI(정답, 클러스터) / H(정답)
     = [H(정답) - H(정답|클러스터)] / H(정답)
```

### 4.3 정규화의 의미
- **분자**: 실제 얻은 정보량
- **분모**: 최대 가능한 정보량
- **결과**: "최대 정보량 중 몇 % 획득했는가"

## 5. NMI의 특성과 해석

### 5.1 점수 범위
- **NMI = 1**: 완벽한 클러스터링 (정답 완전 예측 가능)
- **NMI = 0**: 무의미한 클러스터링 (정답과 무관)
- **0 < NMI < 1**: 부분적으로 유용한 정보 제공

### 5.2 핵심 특성
- **그룹 크기에 안정적**: 불균형 데이터에서도 신뢰성 유지
- **정보 중심 평가**: "얼마나 많은 정보를 전달하는가" 측정
- **대칭성**: NMI(A,B) = NMI(B,A)

## 6. 개념적 핵심 통찰

### 6.1 "정보량 = 불확실성 감소"
NMI의 모든 수학이 이 하나의 직관에서 출발합니다.

### 6.2 "최대 정보량만큼 정보를 제공한다"
완벽한 클러스터링에서는 원래 불확실성만큼의 정보를 정확히 제공합니다.

### 6.3 정보 이론적 관점
클러스터링을 "정보 전달 과정"으로 바라보는 완전히 새로운 시각을 제공합니다.

## 7. ARI와의 철학적 차이

| 관점 | ARI | NMI |
|------|-----|-----|
| **기본 질문** | "쌍의 일치" | "정보 전달" |
| **평가 단위** | 데이터 쌍 | 전체 분포 |
| **수학적 기반** | 조합론 | 정보 이론 |
| **장점** | 직관적, 쌍별 정확도 | 안정적, 정보량 측정 |

## 8. 결론: NMI의 본질

**NMI는 "클러스터링이 정답에 대해 제공하는 정보의 비율"을 측정하는 지표입니다.**

정보 이론의 엄밀한 수학적 기반 위에서, 클러스터링의 품질을 "불확실성 감소량"이라는 직관적이면서도 정확한 방식으로 평가합니다.